{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tutorial link:  \n",
    "    https://towardsdatascience.com/learning-to-perform-linear-filtering-using-natural-image-data-db289d0b0457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the training directory\n",
    "data_dir = 'data'\n",
    "\n",
    "folderpaths = os.path.join(data_dir,'*g')\n",
    "imagepaths = glob.glob(folderpaths)\n",
    "\n",
    "images = []\n",
    "grayimages = []    \n",
    "filteredimages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\flower.jpg\n",
      "data\\house1.jpg\n",
      "data\\house2.jpg\n"
     ]
    }
   ],
   "source": [
    "for imagepath in imagepaths:\n",
    "    print(imagepath)\n",
    "    img = cv2.imread(imagepath)\n",
    "    cv2.imshow(imagepath, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grayscale(img):\n",
    "    #transform color image to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return gray_img\n",
    "\n",
    "def filter_image_sobel_x(img):\n",
    "    #perform filtering to the input image\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    return sobelx\n",
    "\n",
    "def normalize_image255(img):\n",
    "    #changes the input image range from (0, 255) to (0, 1)\n",
    "    img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    return img\n",
    "\n",
    "def normalize_image(img):\n",
    "    #normalizes the input image to range(0, 1) for visualization\n",
    "    img = img - np.min(img)\n",
    "    img = img/np.max(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23449805, 0.23449805, 0.23449805, ..., 0.23060787, 0.23565884,\n",
       "        0.23565884],\n",
       "       [0.23057649, 0.23449805, 0.23449805, ..., 0.23452942, 0.23565884,\n",
       "        0.23565884],\n",
       "       [0.23057649, 0.23057649, 0.23057649, ..., 0.23800394, 0.23913334,\n",
       "        0.23913334],\n",
       "       ...,\n",
       "       [0.2906353 , 0.2906353 , 0.2906353 , ..., 0.19530196, 0.1913804 ,\n",
       "        0.1913804 ],\n",
       "       [0.2906353 , 0.2906353 , 0.2906353 , ..., 0.1913804 , 0.1913804 ,\n",
       "        0.1913804 ],\n",
       "       [0.2906353 , 0.2906353 , 0.2906353 , ..., 0.1913804 , 0.1913804 ,\n",
       "        0.1913804 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for imagepath in imagepaths:\n",
    "    img = cv2.imread(imagepath,cv2.IMREAD_COLOR) #unit-8 image\n",
    "    \n",
    "    img = normalize_image255(img)\n",
    "    gray_img = make_grayscale(img)    \n",
    "    filtered_img = filter_image_sobel_x(gray_img)\n",
    "    \n",
    "    ''' \n",
    "    cv2.imshow('imagepath1', img)\n",
    "    cv2.imshow('imagepath2', gray_img)\n",
    "    cv2.imshow('imagepath3', filtered_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    '''\n",
    "    \n",
    "    images.append(img)\n",
    "    grayimages.append(gray_img)\n",
    "    filteredimages.append(filtered_img)\n",
    "gray_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: (250, 250, 3)\n",
      "grayimages shape: (250, 250, 1)\n",
      "filteredimages shape: (250, 250, 1)\n"
     ]
    }
   ],
   "source": [
    "grayimages = np.expand_dims(grayimages, -1)\n",
    "filteredimages = np.expand_dims(filteredimages, -1)\n",
    "\n",
    "image_no = 102\n",
    "print(\"images shape: {}\".format(images[image_no].shape))\n",
    "print(\"grayimages shape: {}\".format(grayimages[image_no].shape))\n",
    "print(\"filteredimages shape: {}\".format(filteredimages[image_no].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 250)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_height, input_width = gray_img.shape\n",
    "input_height, input_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cnn_model():\n",
    "    #return a convolutional neural network model with a single linear convolution layer\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(1, (3,3), padding='same', input_shape=(input_height, input_width, 1)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 250, 250, 1)       10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "111/111 [==============================] - 9s 77ms/step - loss: 0.1484 - accuracy: 0.1241 - val_loss: 0.1256 - val_accuracy: 0.0990\n",
      "111/111 [==============================] - 7s 61ms/step - loss: 0.0825 - accuracy: 0.1224 - val_loss: 0.0796 - val_accuracy: 0.0973\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0568 - accuracy: 0.1216 - val_loss: 0.0575 - val_accuracy: 0.0968\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0448 - accuracy: 0.1213 - val_loss: 0.0466 - val_accuracy: 0.0966\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0393 - accuracy: 0.1212 - val_loss: 0.0411 - val_accuracy: 0.0965\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0368 - accuracy: 0.1211 - val_loss: 0.0384 - val_accuracy: 0.0964\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0356 - accuracy: 0.1211 - val_loss: 0.0369 - val_accuracy: 0.0964\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0350 - accuracy: 0.1210 - val_loss: 0.0359 - val_accuracy: 0.0964\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0347 - accuracy: 0.1210 - val_loss: 0.0354 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0345 - accuracy: 0.1210 - val_loss: 0.0351 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0344 - accuracy: 0.1210 - val_loss: 0.0350 - val_accuracy: 0.0963\n",
      "  1/111 [..............................] - ETA: 0s - loss: 0.0367 - accuracy: 0.3346WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.382622). Check your callbacks.\n",
      "  2/111 [..............................] - ETA: 1:34 - loss: 0.0303 - accuracy: 0.2099WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.238502). Check your callbacks.\n",
      "111/111 [==============================] - 9s 80ms/step - loss: 0.0343 - accuracy: 0.1210 - val_loss: 0.0348 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0343 - accuracy: 0.1210 - val_loss: 0.0347 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0342 - accuracy: 0.1210 - val_loss: 0.0346 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 60ms/step - loss: 0.0342 - accuracy: 0.1210 - val_loss: 0.0345 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.0341 - accuracy: 0.1210 - val_loss: 0.0344 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0341 - accuracy: 0.1210 - val_loss: 0.0344 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 60ms/step - loss: 0.0340 - accuracy: 0.1210 - val_loss: 0.0343 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 58ms/step - loss: 0.0340 - accuracy: 0.1210 - val_loss: 0.0343 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 58ms/step - loss: 0.0340 - accuracy: 0.1210 - val_loss: 0.0343 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0339 - accuracy: 0.1210 - val_loss: 0.0342 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 65ms/step - loss: 0.0339 - accuracy: 0.1210 - val_loss: 0.0342 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0338 - accuracy: 0.1210 - val_loss: 0.0341 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 58ms/step - loss: 0.0338 - accuracy: 0.1210 - val_loss: 0.0341 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 65ms/step - loss: 0.0338 - accuracy: 0.1210 - val_loss: 0.0340 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 61ms/step - loss: 0.0338 - accuracy: 0.1210 - val_loss: 0.0340 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 64ms/step - loss: 0.0337 - accuracy: 0.1210 - val_loss: 0.0340 - val_accuracy: 0.0963\n",
      "  1/111 [..............................] - ETA: 0s - loss: 0.0472 - accuracy: 0.0529WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.332983). Check your callbacks.\n",
      "  2/111 [..............................] - ETA: 48s - loss: 0.0316 - accuracy: 0.0875WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.170530). Check your callbacks.\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.0337 - accuracy: 0.1210 - val_loss: 0.0340 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 58ms/step - loss: 0.0337 - accuracy: 0.1210 - val_loss: 0.0340 - val_accuracy: 0.0963\n",
      "  2/111 [..............................] - ETA: 1:22 - loss: 0.0231 - accuracy: 0.1382WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.729124). Check your callbacks.\n",
      "111/111 [==============================] - 11s 102ms/step - loss: 0.0337 - accuracy: 0.1210 - val_loss: 0.0339 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0336 - accuracy: 0.1210 - val_loss: 0.0339 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0336 - accuracy: 0.1210 - val_loss: 0.0339 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0336 - accuracy: 0.1210 - val_loss: 0.0338 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0336 - accuracy: 0.1210 - val_loss: 0.0338 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 61ms/step - loss: 0.0335 - accuracy: 0.1210 - val_loss: 0.0338 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 60ms/step - loss: 0.0335 - accuracy: 0.1210 - val_loss: 0.0338 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 59ms/step - loss: 0.0335 - accuracy: 0.1210 - val_loss: 0.0337 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 64ms/step - loss: 0.0335 - accuracy: 0.1210 - val_loss: 0.0337 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0335 - accuracy: 0.1210 - val_loss: 0.0337 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0334 - accuracy: 0.1210 - val_loss: 0.0337 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0334 - accuracy: 0.1210 - val_loss: 0.0337 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 60ms/step - loss: 0.0334 - accuracy: 0.1210 - val_loss: 0.0336 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0334 - accuracy: 0.1210 - val_loss: 0.0336 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 9s 81ms/step - loss: 0.0334 - accuracy: 0.1210 - val_loss: 0.0336 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 59ms/step - loss: 0.0333 - accuracy: 0.1210 - val_loss: 0.0336 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 57ms/step - loss: 0.0333 - accuracy: 0.1210 - val_loss: 0.0336 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 9s 78ms/step - loss: 0.0333 - accuracy: 0.1210 - val_loss: 0.0336 - val_accuracy: 0.0963\n",
      "  2/111 [..............................] - ETA: 3:04 - loss: 0.0278 - accuracy: 0.0697WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.711752). Check your callbacks.\n",
      "111/111 [==============================] - 19s 167ms/step - loss: 0.0333 - accuracy: 0.1210 - val_loss: 0.0335 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0333 - accuracy: 0.1210 - val_loss: 0.0335 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 60ms/step - loss: 0.0333 - accuracy: 0.1210 - val_loss: 0.0335 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 55ms/step - loss: 0.0333 - accuracy: 0.1210 - val_loss: 0.0335 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0332 - accuracy: 0.1210 - val_loss: 0.0335 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 11s 96ms/step - loss: 0.0332 - accuracy: 0.1210 - val_loss: 0.0335 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 10s 87ms/step - loss: 0.0332 - accuracy: 0.1210 - val_loss: 0.0335 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0332 - accuracy: 0.1210 - val_loss: 0.0334 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.0332 - accuracy: 0.1210 - val_loss: 0.0334 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.0332 - accuracy: 0.1210 - val_loss: 0.0334 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 77ms/step - loss: 0.0332 - accuracy: 0.1210 - val_loss: 0.0334 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0334 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 61ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0334 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 14s 122ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0334 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 9s 78ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0334 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 58ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "  2/111 [..............................] - ETA: 1:54 - loss: 0.0333 - accuracy: 0.0836WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.032165). Check your callbacks.\n",
      "111/111 [==============================] - 16s 147ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 9s 81ms/step - loss: 0.0331 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 64ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 61ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 65ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 64ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0333 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 59ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0330 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 65ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "  1/111 [..............................] - ETA: 0s - loss: 0.0369 - accuracy: 0.2146WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.127626). Check your callbacks.\n",
      "111/111 [==============================] - 8s 76ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 59ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 6s 58ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 61ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 64ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0331 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0331 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0331 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0332 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 9s 84ms/step - loss: 0.0329 - accuracy: 0.1210 - val_loss: 0.0331 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 62ms/step - loss: 0.0328 - accuracy: 0.1210 - val_loss: 0.0331 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0328 - accuracy: 0.1210 - val_loss: 0.0331 - val_accuracy: 0.0963\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.0328 - accuracy: 0.1210 - val_loss: 0.0331 - val_accuracy: 0.0963\n"
     ]
    }
   ],
   "source": [
    "model = linear_cnn_model()\n",
    "sgd = optimizers.SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "number_of_epochs = 100\n",
    "loss = []\n",
    "val_loss = []\n",
    "convweights = []\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    history_temp = model.fit(grayimages, filteredimages,\n",
    "                            batch_size = 4,\n",
    "                            epochs = 1,\n",
    "                            validation_split = 0.2)\n",
    "    loss.append(history_temp.history['loss'][0])\n",
    "    val_loss.append(history_temp.history['val_loss'][0])\n",
    "    convweights.append(model.layers[0].get_weights()[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "model.save_weights(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest model checkpoint is:  training_1\\cp.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x21e2f28aac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "print(\"latest model checkpoint is: \",latest)\n",
    "model = linear_cnn_model()\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "plt.close('all')\n",
    "    \n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Training loss', 'Validation loss'], loc='upper right')\n",
    "plt.show()\n",
    "plt.savefig('trainingvalidationlossgx.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/weightfigure0.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure1.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure2.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure3.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure4.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure5.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure6.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure7.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure8.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure9.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure10.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure11.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure12.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure13.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure14.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure15.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure16.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure17.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure18.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure19.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure20.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure21.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure22.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure23.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure24.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure25.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure26.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure27.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure28.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure29.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure30.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure31.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure32.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure33.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure34.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure35.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure36.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure37.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure38.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure39.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure40.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure41.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure42.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure43.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure44.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure45.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure46.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure47.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure48.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure49.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure50.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure51.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure52.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure53.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure54.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure55.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure56.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure57.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure58.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure59.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure60.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure61.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure62.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure63.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure64.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure65.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure66.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure67.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure68.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure69.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/weightfigure70.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure71.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure72.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure73.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure74.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure75.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure76.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure77.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure78.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure79.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure80.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure81.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure82.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure83.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure84.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure85.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure86.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure87.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure88.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure89.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure90.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure91.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure92.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure93.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure94.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure95.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure96.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure97.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure98.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n",
      "images/weightfigure99.png\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "def visualize_matrix(M, epoch=1):\n",
    "    \"\"\"\n",
    "    Create a visualization of an arbitrary matrix.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    \n",
    "    title = \"Epoch {}\".format(epoch)\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    \n",
    "    height, width = M.shape\n",
    "    Mud = np.flipud(M) # Now the i-index complies with matplotlib axes\n",
    "    coordinates = [(i,j) for i in range(height) for j in range(width)]\n",
    "    print(coordinates)\n",
    "    for coordinate in coordinates:\n",
    "        i,j = coordinate\n",
    "        value = np.round(Mud[i,j], decimals=2)\n",
    "        relcoordinate = (j/float(width), i/float(height))\n",
    "        ax1.annotate(value, relcoordinate, ha='left', va='center',\n",
    "                     size=22, alpha=0.7, family='serif')\n",
    "        \n",
    "    padding = 0.25\n",
    "    wmargin = (width-1)/float(width) + padding\n",
    "    hmargin = (height-1)/float(height) + padding\n",
    "    \n",
    "    hcenter = np.median(range(height))/float(height)\n",
    "    print(hcenter)\n",
    "    hcenter = hcenter + 0.015 # Offset due to the character alignment\n",
    "    \n",
    "    bracket_d = 0.4\n",
    "    bracket_b = 0.05\n",
    "    bracket_paddingl = 0.05\n",
    "    bracket_paddingr = -0.05\n",
    "    \n",
    "    ax1.plot([-bracket_paddingl, -bracket_paddingl],[hcenter-bracket_d, hcenter+bracket_d], 'k-', lw=2, alpha=0.7)\n",
    "    ax1.plot([-bracket_paddingl, -bracket_paddingl+bracket_b], [hcenter-bracket_d, hcenter-bracket_d], 'k-', lw=2, alpha=0.7)\n",
    "    ax1.plot([-bracket_paddingl, -bracket_paddingl+bracket_b], [hcenter+bracket_d, hcenter+bracket_d], 'k-', lw=2, alpha=0.7)\n",
    "    \n",
    "    ax1.plot([wmargin-bracket_paddingr, wmargin-bracket_paddingr],[hcenter-bracket_d, hcenter+bracket_d], 'k-', lw=2, alpha=0.7)\n",
    "    ax1.plot([wmargin-bracket_paddingr-bracket_b, wmargin-bracket_paddingr], [hcenter-bracket_d, hcenter-bracket_d], 'k-', lw=2, alpha=0.7)\n",
    "    ax1.plot([wmargin-bracket_paddingr-bracket_b, wmargin-bracket_paddingr], [hcenter+bracket_d, hcenter+bracket_d], 'k-', lw=2, alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlim([-padding, wmargin+0.06])\n",
    "    ax1.set_ylim([-padding, hmargin])\n",
    "    \n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    matshowplt = ax2.matshow(M, cmap='gray', vmin=-2, vmax=2)\n",
    "    cbar = plt.colorbar(matshowplt, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    cbar.ax.tick_params(labelsize=18) \n",
    "    cbar.ax.get_yaxis().labelpad = 20\n",
    "    cbar.ax.set_ylabel('Weight value', rotation=270, fontsize=20)\n",
    "    ax2.get_xaxis().set_visible(False)\n",
    "    ax2.get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "savefolder = 'images/'\n",
    "\n",
    "for i in range(len(convweights)):\n",
    "    savepath = savefolder+'weightfigure'+str(i)+'.png'\n",
    "    print(savepath)\n",
    "    M = convweights[i]\n",
    "    fig = visualize_matrix(M, epoch=i+1)\n",
    "    fig.savefig(savepath)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from natsort import natsorted\n",
    "\n",
    "# Get the paths to the convolution weight visualization images\n",
    "image_dir = 'images/'\n",
    "\n",
    "imagepaths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.png')]\n",
    "imagepaths = natsorted(imagepaths)\n",
    "\n",
    "with imageio.get_writer('weightmoviegx.gif', mode='I') as writer:\n",
    "    for impath in imagepaths:\n",
    "        image = imageio.imread(impath)\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_img = model.predict(np.array([np.expand_dims(gray_img, -1)]))\n",
    "predicted_img = predicted_img.squeeze()\n",
    "        \n",
    "margin_img = np.ones(shape=(250, 10, 3))\n",
    "combined_image = np.hstack((np.dstack((normalize_image(predicted_img),)*3), margin_img, np.dstack((normalize_image(filtered_img),)*3)))\n",
    "\n",
    "cv2.imwrite('PredictedFiltered_sobelx.png', (255.0*combined_image).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctensor2",
   "language": "python",
   "name": "ctensor2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
